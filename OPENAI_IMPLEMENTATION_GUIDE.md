# OpenAI í†µí•© êµ¬í˜„ ê°€ì´ë“œ

ì‹¤ì œ ì½”ë“œ ë³€ê²½ ì‚¬í•­ì„ í¬í•¨í•œ ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œì…ë‹ˆë‹¤.

## 1ë‹¨ê³„: ì˜ì¡´ì„± ì¶”ê°€

`app/requirements.txt`ì— ì¶”ê°€:
```
langchain-openai>=0.1.0
```

## 2ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ ë˜ëŠ” Docker í™˜ê²½ ë³€ìˆ˜ì— ì¶”ê°€:
```bash
OPENAI_API_KEY=sk-your-api-key-here
```

## 3ë‹¨ê³„: ì½”ë“œ ë³€ê²½

### 3.1 `app/app.py` ìˆ˜ì •

```python
"""LangChain Hello World ì•± - pgvector ì—°ë™ ì˜ˆì œ."""

import os
import time
from typing import List

from langchain_core.documents import Document
# ë³€ê²½: FakeEmbeddings ëŒ€ì‹  OpenAIEmbeddings ì‚¬ìš©
from langchain_openai import OpenAIEmbeddings
from langchain_postgres import PGVector


def get_vector_store():
    """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        PGVector ë²¡í„° ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤.
    """
    db_host = os.getenv("POSTGRES_HOST", "postgres")
    db_port = os.getenv("POSTGRES_PORT", "5432")
    db_user = os.getenv("POSTGRES_USER", "langchain")
    db_password = os.getenv("POSTGRES_PASSWORD", "langchain")
    db_name = os.getenv("POSTGRES_DB", "langchain")

    connection_string = (
        f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    )

    # ë³€ê²½: OpenAI Embeddings ì‚¬ìš©
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    vector_store = PGVector(
        embeddings,
        connection=connection_string,
    )
    return vector_store
```

### 3.2 `rag_chain.py` ìƒˆë¡œ ìƒì„±

```python
"""RAG ì²´ì¸ êµ¬ì„± ëª¨ë“ˆ."""

from typing import List
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser


def create_rag_chain(llm, retriever):
    """RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        llm: LangChain LLM ì¸ìŠ¤í„´ìŠ¤
        retriever: ê²€ìƒ‰ê¸° (Retriever)

    Returns:
        RAG ì²´ì¸
    """
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ì˜ ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}"""),
        ("human", "{question}")
    ])

    # ì²´ì¸ êµ¬ì„±
    chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    return chain
```

### 3.3 `api_server.py` í™•ì¥

```python
"""FastAPI ì„œë²„ - LangChain RAG ì‹œìŠ¤í…œ API."""

import os
from typing import List, Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from .app import get_vector_store, test_pgvector, wait_for_postgres
from .rag_chain import create_rag_chain
from langchain_openai import ChatOpenAI

app = FastAPI(title="LangChain RAG API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class SearchRequest(BaseModel):
    query: str
    k: int = 5

class ChatRequest(BaseModel):
    query: str

class DocumentResponse(BaseModel):
    content: str
    metadata: dict
    score: Optional[float] = None

class SearchResponse(BaseModel):
    results: List[DocumentResponse]
    query: str
    total: int

class ChatResponse(BaseModel):
    answer: str
    query: str
    model: str

# ì „ì—­ ë³€ìˆ˜
_vector_store = None
_llm = None
_rag_chain = None

def get_vector_store_instance():
    """ë²¡í„° ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    global _vector_store
    if _vector_store is None:
        _vector_store = get_vector_store()
    return _vector_store

def get_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    global _llm
    if _llm is None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        _llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            openai_api_key=api_key
        )
    return _llm

def get_rag_chain():
    """RAG ì²´ì¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    global _rag_chain
    if _rag_chain is None:
        vector_store = get_vector_store_instance()
        retriever = vector_store.as_retriever(search_kwargs={"k": 5})
        llm = get_llm()
        _rag_chain = create_rag_chain(llm, retriever)
    return _rag_chain

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”."""
    print("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")

    db_host = os.getenv("POSTGRES_HOST", "postgres")
    db_port = os.getenv("POSTGRES_PORT", "5432")
    db_user = os.getenv("POSTGRES_USER", "langchain")
    db_password = os.getenv("POSTGRES_PASSWORD", "langchain")
    db_name = os.getenv("POSTGRES_DB", "langchain")

    connection_string = (
        f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    )

    wait_for_postgres(connection_string)
    test_pgvector(connection_string)
    get_vector_store_instance()
    print("âœ… FastAPI ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """RAG ê¸°ë°˜ ì±—ë´‡ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        request: ì±—ë´‡ ìš”ì²­

    Returns:
        ì±—ë´‡ ì‘ë‹µ
    """
    try:
        rag_chain = get_rag_chain()
        answer = rag_chain.invoke(request.query)

        return ChatResponse(
            answer=answer,
            query=request.query,
            model="gpt-4o-mini"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/rag")
async def rag(request: ChatRequest):
    """ê²€ìƒ‰ ê²°ê³¼ì™€ LLM ë‹µë³€ì„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        request: ì±—ë´‡ ìš”ì²­

    Returns:
        ê²€ìƒ‰ ê²°ê³¼ì™€ LLM ë‹µë³€
    """
    try:
        vector_store = get_vector_store_instance()

        # 1. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_results = vector_store.similarity_search_with_score(
            request.query, k=5
        )

        # 2. LLM ë‹µë³€ ìƒì„±
        rag_chain = get_rag_chain()
        answer = rag_chain.invoke(request.query)

        return {
            "answer": answer,
            "sources": [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score)
                }
                for doc, score in search_results
            ],
            "query": request.query
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ë“¤...
```

### 3.4 í”„ë¡ íŠ¸ì—”ë“œ ìˆ˜ì • (`frontend/app/page.tsx`)

```typescript
// ê¸°ì¡´ ê²€ìƒ‰ ëŒ€ì‹  ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
const handleSearch = async (searchQuery: string) => {
  // ... ê¸°ì¡´ ì½”ë“œ ...

  try {
    // ì˜µì…˜ 1: ì±„íŒ…ë§Œ ì‚¬ìš©
    const response = await fetch(
      `${process.env.NEXT_PUBLIC_API_URL || "http://localhost:8000"}/chat`,
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          query: searchQuery,
        }),
      }
    );

    // ì˜µì…˜ 2: ê²€ìƒ‰ + ì±„íŒ… í•¨ê»˜ ì‚¬ìš©
    // const response = await fetch(
    //   `${process.env.NEXT_PUBLIC_API_URL || "http://localhost:8000"}/rag`,
    //   ...
    // );

    const data = await response.json();

    // ì‘ë‹µ ì²˜ë¦¬
    const assistantMessage: Message = {
      id: (Date.now() + 1).toString(),
      type: "assistant",
      content: data.answer, // LLM ë‹µë³€
      timestamp: new Date(),
      sources: data.sources, // ê²€ìƒ‰ ê²°ê³¼ (ì˜µì…˜)
    };
    setMessages((prev) => [...prev, assistantMessage]);
  } catch (err) {
    // ì—ëŸ¬ ì²˜ë¦¬
  }
};
```

## 4ë‹¨ê³„: Docker í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€

`docker-compose.yaml` ìˆ˜ì •:
```yaml
langchain-app:
  # ... ê¸°ì¡´ ì„¤ì • ...
  environment:
    POSTGRES_HOST: postgres
    POSTGRES_PORT: 5432
    POSTGRES_USER: langchain
    POSTGRES_PASSWORD: langchain
    POSTGRES_DB: langchain
    OPENAI_API_KEY: ${OPENAI_API_KEY}  # ì¶”ê°€
```

`.env` íŒŒì¼ ìƒì„± (í”„ë¡œì íŠ¸ ë£¨íŠ¸):
```
OPENAI_API_KEY=sk-your-api-key-here
```

## í…ŒìŠ¤íŠ¸

1. **Embeddings í…ŒìŠ¤íŠ¸**:
```bash
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{"query": "LangChain", "k": 3}'
```

2. **ì±„íŒ… í…ŒìŠ¤íŠ¸**:
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?"}'
```

3. **í†µí•© í…ŒìŠ¤íŠ¸**:
```bash
curl -X POST http://localhost:8000/rag \
  -H "Content-Type: application/json" \
  -d '{"query": "LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?"}'
```

## ë¬¸ì œ í•´ê²°

### API í‚¤ ì˜¤ë¥˜
- í™˜ê²½ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
- Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í™˜ê²½ ë³€ìˆ˜ í™•ì¸: `docker exec langchain-app env | grep OPENAI`

### ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜
- ê¸°ì¡´ FakeEmbeddings(384ì°¨ì›)ë¡œ ìƒì„±ëœ ë²¡í„°ëŠ” ì¬ì¸ë±ì‹± í•„ìš”
- ë˜ëŠ” ìƒˆë¡œìš´ í…Œì´ë¸” ìƒì„± í›„ ë§ˆì´ê·¸ë ˆì´ì…˜

### ë¹„ìš© ê´€ë¦¬
- OpenAI ëŒ€ì‹œë³´ë“œì—ì„œ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- Rate limiting êµ¬í˜„
- ìºì‹± ì „ëµ ë„ì…

