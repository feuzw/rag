"""FastAPI ì„œë²„ - LangChain RAG ì‹œìŠ¤í…œ API."""

import os
from typing import List, Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
import psycopg2
from sqlalchemy.exc import DataError

# í™˜ê²½ì— ë”°ë¼ ìƒëŒ€/ì ˆëŒ€ import ì„ íƒ
# ë¡œì»¬: app/ í´ë”ê°€ ìˆìœ¼ë©´ ìƒëŒ€ import ì‚¬ìš©
# ìš°ë¶„íˆ¬: app/ í´ë”ê°€ ì—†ê³  ë£¨íŠ¸ì— íŒŒì¼ë“¤ì´ ì§ì ‘ ìˆìœ¼ë©´ ì ˆëŒ€ import ì‚¬ìš©
try:
    from .app import get_vector_store, test_pgvector, wait_for_postgres
    from .models import (
        get_llm_provider,
        set_llm_provider,
        LLMProvider,
    )
    # ChatMidmì€ ì„ íƒì  import (ìš°ë¶„íˆ¬ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
    try:
        from .models import ChatMidm
    except ImportError:
        ChatMidm = None
    from .router.rag_router import router as rag_router
    from .router.chat_router import router as chat_router
except ImportError:
    # ìš°ë¶„íˆ¬ í™˜ê²½: ì ˆëŒ€ import ì‚¬ìš©
    from app import get_vector_store, test_pgvector, wait_for_postgres
    from models import (
        get_llm_provider,
        set_llm_provider,
        LLMProvider,
    )
    # ChatMidmì€ ì„ íƒì  import (ìš°ë¶„íˆ¬ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
    try:
        from models import ChatMidm
    except ImportError:
        ChatMidm = None
    from router.rag_router import router as rag_router
    from router.chat_router import router as chat_router

app = FastAPI(title="LangChain RAG API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¼ìš°í„° ë“±ë¡
app.include_router(rag_router)
app.include_router(chat_router)


# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class SearchRequest(BaseModel):
    """ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸."""

    query: str
    k: int = 5


class DocumentResponse(BaseModel):
    """ë¬¸ì„œ ì‘ë‹µ ëª¨ë¸."""

    content: str
    metadata: dict
    score: Optional[float] = None


class SearchResponse(BaseModel):
    """ê²€ìƒ‰ ì‘ë‹µ ëª¨ë¸."""

    results: List[DocumentResponse]
    query: str
    total: int


class AddDocumentRequest(BaseModel):
    """ë¬¸ì„œ ì¶”ê°€ ìš”ì²­ ëª¨ë¸."""

    content: str
    metadata: Optional[dict] = None


class AddDocumentsRequest(BaseModel):
    """ì—¬ëŸ¬ ë¬¸ì„œ ì¶”ê°€ ìš”ì²­ ëª¨ë¸."""

    documents: List[AddDocumentRequest]


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­ ëª¨ë¸."""

    query: str


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ ëª¨ë¸."""

    answer: str
    query: str
    model: str


class SearchAndChatResponse(BaseModel):
    """ê²€ìƒ‰ ë° ì±„íŒ… í†µí•© ì‘ë‹µ ëª¨ë¸."""

    answer: str
    sources: List[DocumentResponse]
    query: str


# ì „ì—­ ë³€ìˆ˜
_vector_store = None
_llm_initialized = False


def get_vector_store_instance():
    """ë²¡í„° ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    global _vector_store
    if _vector_store is None:
        _vector_store = get_vector_store()
    return _vector_store


def _initialize_default_llm() -> ChatOpenAI:
    """ê¸°ë³¸ LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    Returns:
        ì´ˆê¸°í™”ëœ ChatOpenAI ì¸ìŠ¤í„´ìŠ¤.

    Raises:
        ValueError: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°.
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.7,
        openai_api_key=api_key,
    )


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”."""
    print("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")

    # Neon DB ë˜ëŠ” ê¸°íƒ€ PostgreSQL ì—°ê²° ë¬¸ìì—´ ì‚¬ìš©
    connection_string = os.getenv("POSTGRES_CONNECTION_STRING")

    if not connection_string:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ê°œë³„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì¡°í•© (í•˜ìœ„ í˜¸í™˜ì„±)
        db_host = os.getenv("POSTGRES_HOST", "postgres")
        db_port = os.getenv("POSTGRES_PORT", "5432")
        db_user = os.getenv("POSTGRES_USER", "langchain")
        db_password = os.getenv("POSTGRES_PASSWORD", "langchain")
        db_name = os.getenv("POSTGRES_DB", "langchain")
        connection_string = (
            f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
        )

    # PostgreSQL ì—°ê²° ëŒ€ê¸°
    wait_for_postgres(connection_string)

    # pgvector í™•ì¥ í™•ì¸
    test_pgvector(connection_string)

    # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
    get_vector_store_instance()

    # LLM í”„ë¡œë°”ì´ë” ì´ˆê¸°í™” (í•œë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ì²´í¬)
    global _llm_initialized
    if not _llm_initialized:
        provider = get_llm_provider()
        try:
            # ì´ë¯¸ ì£¼ì…ëœ LLMì´ ìˆëŠ”ì§€ í™•ì¸
            _ = provider.get_llm()
            print("âœ… ì£¼ì…ëœ LLM ì‚¬ìš© ì¤‘")
            _llm_initialized = True
        except ValueError:
            # í™˜ê²½ ë³€ìˆ˜ì—ì„œ LLM í”„ë¡œë°”ì´ë” í™•ì¸
            llm_provider = os.getenv("LLM_PROVIDER", "openai").lower()

            if llm_provider == "midm":
                # Mi:dm ëª¨ë¸ ì‚¬ìš©
                if ChatMidm is None:
                    print("âš ï¸  ChatMidmì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OpenAIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    llm_provider = "openai"
                else:
                    try:
                        local_model_dir = os.getenv("LOCAL_MODEL_DIR")
                        midm_model = ChatMidm(
                            model_path=local_model_dir,  # Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
                            temperature=0.7,
                            max_tokens=512,
                        )
                        provider.set_llm(midm_model)
                        print(f"âœ… Mi:dm ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ! (ê²½ë¡œ: {local_model_dir or 'ê¸°ë³¸ ê²½ë¡œ'})")
                        _llm_initialized = True
                    except Exception as e:
                        print(f"âš ï¸  Mi:dm ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                        print("   ì±„íŒ… ê¸°ëŠ¥ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")

            # OpenAI ì‚¬ìš© (midm ì‹¤íŒ¨ ì‹œ ë˜ëŠ” ê¸°ë³¸ê°’)
            if llm_provider != "midm" or ChatMidm is None or not _llm_initialized:
                # ê¸°ë³¸ LLM (OpenAI) ì´ˆê¸°í™” ì‹œë„
                try:
                    default_llm = _initialize_default_llm()
                    provider.set_llm(default_llm)
                    print("âœ… ê¸°ë³¸ LLM (OpenAI) ì´ˆê¸°í™” ì™„ë£Œ!")
                    _llm_initialized = True
                except ValueError as e:
                    print(f"âš ï¸  LLM ì´ˆê¸°í™” ì‹¤íŒ¨ (API í‚¤ ì—†ìŒ): {e}")
                    print("   ì±„íŒ… ê¸°ëŠ¥ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ì§€ë§Œ ê²€ìƒ‰ ê¸°ëŠ¥ì€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
                    print("   LLMì„ ì£¼ì…í•˜ë ¤ë©´ set_llm_provider()ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    else:
        print("âœ… LLMì€ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (í•™ìŠµëœ ë‚´ìš© ë³´ì¡´)")

    print("âœ… FastAPI ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸."""
    return {"message": "LangChain RAG API", "status": "running"}


@app.get("/health")
async def health():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸."""
    return {"status": "healthy"}


@app.post("/search", response_model=SearchResponse)
async def search(request: SearchRequest):
    """ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        request: ê²€ìƒ‰ ìš”ì²­.

    Returns:
        ê²€ìƒ‰ ê²°ê³¼.
    """
    try:
        vector_store = get_vector_store_instance()
        results = vector_store.similarity_search_with_score(
            request.query, k=request.k
        )

        document_responses = [
            DocumentResponse(
                content=doc.page_content,
                metadata=doc.metadata,
                score=float(score),
            )
            for doc, score in results
        ]

        return SearchResponse(
            results=document_responses,
            query=request.query,
            total=len(document_responses),
        )
    except (DataError, psycopg2.errors.DataException) as e:
        error_msg = str(e)
        if "different vector dimensions" in error_msg:
            raise HTTPException(
                status_code=400,
                detail=(
                    "ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜ ì˜¤ë¥˜: ì €ì¥ëœ ë²¡í„°ì™€ í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸ì˜ ì°¨ì›ì´ ë‹¤ë¦…ë‹ˆë‹¤. "
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ë ¤ë©´ /reset-collection ì—”ë“œí¬ì¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”."
                ),
            )
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/add-document")
async def add_document(request: AddDocumentRequest):
    """ë‹¨ì¼ ë¬¸ì„œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

    Args:
        request: ë¬¸ì„œ ì¶”ê°€ ìš”ì²­.

    Returns:
        ì¶”ê°€ ê²°ê³¼.
    """
    try:
        try:
            from .service.embedding_ingest_service import add_document as add_doc_service
        except ImportError:
            from service.embedding_ingest_service import add_document as add_doc_service
        return add_doc_service(request.content, request.metadata)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/add-documents")
async def add_documents(request: AddDocumentsRequest):
    """ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

    Args:
        request: ë¬¸ì„œ ì¶”ê°€ ìš”ì²­.

    Returns:
        ì¶”ê°€ ê²°ê³¼.
    """
    try:
        try:
            from .service.embedding_ingest_service import add_documents as add_docs_service
        except ImportError:
            from service.embedding_ingest_service import add_documents as add_docs_service
        documents = [
            {"content": doc.content, "metadata": doc.metadata or {}}
            for doc in request.documents
        ]
        return add_docs_service(documents)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))




@app.post("/reset-collection")
async def reset_collection():
    """ë²¡í„° ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    ì£¼ì˜: ì´ ì‘ì—…ì€ ëª¨ë“  ì €ì¥ëœ ë¬¸ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.

    Returns:
        ì´ˆê¸°í™” ê²°ê³¼.
    """
    try:
        # Neon DB ë˜ëŠ” ê¸°íƒ€ PostgreSQL ì—°ê²° ë¬¸ìì—´ ì‚¬ìš©
        connection_string = os.getenv("POSTGRES_CONNECTION_STRING")

        if not connection_string:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ê°œë³„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì¡°í•© (í•˜ìœ„ í˜¸í™˜ì„±)
            db_host = os.getenv("POSTGRES_HOST", "postgres")
            db_port = os.getenv("POSTGRES_PORT", "5432")
            db_user = os.getenv("POSTGRES_USER", "langchain")
            db_password = os.getenv("POSTGRES_PASSWORD", "langchain")
            db_name = os.getenv("POSTGRES_DB", "langchain")
            connection_string = (
                f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
            )

        # PostgreSQL ì—°ê²°
        conn = psycopg2.connect(connection_string)
        conn.autocommit = True
        cursor = conn.cursor()

        # langchain_pg_embedding í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ
        cursor.execute("DELETE FROM langchain_pg_embedding;")
        deleted_count = cursor.rowcount

        # langchain_pg_collection í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ
        cursor.execute("DELETE FROM langchain_pg_collection;")
        collection_count = cursor.rowcount

        cursor.close()
        conn.close()

        # ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
        global _vector_store
        _vector_store = None

        return {
            "message": "ì»¬ë ‰ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "status": "success",
            "deleted_embeddings": deleted_count,
            "deleted_collections": collection_count,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)

